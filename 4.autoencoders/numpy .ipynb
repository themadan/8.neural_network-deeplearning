{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "numpy .ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYVH9K1p42-X",
        "colab_type": "text"
      },
      "source": [
        "[Blog and code for Auto encoder (maile tal gareko yahi blog bata ho)](https://blog.keras.io/building-autoencoders-in-keras.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-qjYLKm5KH",
        "colab_type": "text"
      },
      "source": [
        "# AUTO ENCODER \n",
        "* Unsupervise machine  Learning similar to the pca.\n",
        "* it minimize the same objective function as pca.\n",
        "* It is a neural network.\n",
        "* The neural network's target output us its input.\n",
        "* Reconstruct the input \n",
        "* Label are input\n",
        "* Dimentionality reduction\n",
        "\n",
        "#  PCA VS AUTOENCODER \n",
        "* Auto encoder can learn non-linear transform unlike PCA.\n",
        "* An autoencoder doesn't have to learn dense layers. \n",
        "* More efficiency to learn several layers with an autoencoder.\n",
        "* It gives a representation as the output of each layer.\n",
        "\n",
        "# WHY AUTOENCODER\n",
        "* Data Denosing .\n",
        "* More accurate then pca.\n",
        "\n",
        "# WHAT IS AUTOENCODER \n",
        "* An autoencoder neural network is an unsupervise machine learning algorithm that applies backpropagation ,setting the target values to be equal to the inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cddr8kKHqRSs",
        "colab_type": "text"
      },
      "source": [
        "# There are 4 hyperparameter that we need to set before training an autoencoder.\n",
        "\n",
        "* Code:The number of nodes in the middle layer.small size results in more compression.\n",
        "\n",
        "* Number of layer :encoder ra decoder ma vayako number of layer input ra output bahek.\n",
        "* Loss function:either mean squared error or binary crossentropy.\n",
        "if the input id the range of [0,1] then we typically use crossentropy ,otherwise we use the mean square error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58HN5EGKmXRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1074c92b-7f46-46ed-8d28-4351dee427f7"
      },
      "source": [
        "#!pip install tensorflow==2.0.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from  tensorflow.keras.layers  import Input,Dense\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odj1qNGpvPlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37OyOtDUvYQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f2345485-8985-4f7d-ada5-839cee4909f9"
      },
      "source": [
        "(x_train,_),(x_test,_)=mnist.load_data()\n",
        "x_train=x_train.astype('float32')/255\n",
        "x_test=x_test.astype('float32')/255\n",
        "x_train=x_train.reshape((len(x_train),np.prod(x_train.shape[1:]))) # flatten the input image 28*28 into 784 size.\n",
        "x_test=x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))\n",
        "print (x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfS1U4xQzDQt",
        "colab_type": "text"
      },
      "source": [
        "# single fully-connected neural layer as encoder and decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wVbXUaEmXRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size=784\n",
        "hidden_size=128\n",
        "encoding_dim=32 # this is the size of our encoded representation\n",
        "input_img=Input(shape=(input_size,))\n",
        "encoded=Dense(encoding_dim,activation='relu')(input_img)\n",
        "\n",
        "decoded=Dense(input_size,activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder=Model(input_img,decoded)\n",
        "autoencoder.compile(optimizer='adam',loss='binary_crossentropy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fCvxDgizaOa",
        "colab_type": "text"
      },
      "source": [
        "# Seperate encoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWxNIRj6zljH",
        "colab_type": "text"
      },
      "source": [
        "# model maps an input to its encoded representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eQ9xfcxzSeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder=Model(input_img,encoded)  # This model maps an input to its encoded  representation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsntboEz38l",
        "colab_type": "text"
      },
      "source": [
        "# As wel as the decoder model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPBYOKcqz3DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_input=Input(shape=(encoding_dim,))\n",
        "decoder_layer=autoencoder.layers[-1]\n",
        "decoder=Model(encoded_input,decoder_layer(encoded_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70epxPJoxiIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b3985bb-3483-44e0-a4f7-836958404b12"
      },
      "source": [
        "autoencoder.fit(x_train,x_train,epochs=50,batch_size=256,shuffle=True,validation_data=(x_test,x_test))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.2780 - val_loss: 0.1911\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1715 - val_loss: 0.1541\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1445 - val_loss: 0.1335\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1278 - val_loss: 0.1202\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1171 - val_loss: 0.1119\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1102 - val_loss: 0.1061\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1053 - val_loss: 0.1021\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1017 - val_loss: 0.0990\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0992 - val_loss: 0.0968\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0974 - val_loss: 0.0953\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0962 - val_loss: 0.0944\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0954 - val_loss: 0.0938\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0949 - val_loss: 0.0934\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0946 - val_loss: 0.0931\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0943 - val_loss: 0.0928\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.0941 - val_loss: 0.0927\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0939 - val_loss: 0.0925\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0938 - val_loss: 0.0924\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0937 - val_loss: 0.0923\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0936 - val_loss: 0.0922\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0935 - val_loss: 0.0923\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0934 - val_loss: 0.0922\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0933 - val_loss: 0.0920\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0933 - val_loss: 0.0921\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0932 - val_loss: 0.0920\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0932 - val_loss: 0.0919\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0931 - val_loss: 0.0919\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0931 - val_loss: 0.0919\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0931 - val_loss: 0.0920\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0930 - val_loss: 0.0920\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0927 - val_loss: 0.0917\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0927 - val_loss: 0.0914\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0926 - val_loss: 0.0915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f92e3913dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXnYxIzHmXRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_imgs=encoder.predict(x_test)\n",
        "decoded_imgs=decoder.predict(encoded_imgs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQtSnt9EmXRu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "952ff1d1-c453-4fcf-a706-0c96bda6d2d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "n=10\n",
        "plt.figure(figsize=(20,4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax=plt.subplot(2,n,i+1)\n",
        "  plt.imshow(x_test[i].reshape(28,28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax=plt.subplot(2,n,i+1+n)\n",
        "  plt.imshow(decoded_imgs[i].reshape(28,28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAADnCAYAAACkCqtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSSom4Rngj1kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIE-Dmr62-xz",
        "colab_type": "text"
      },
      "source": [
        "#Adding a sparsity constraint on the encoded representations\n",
        "In the previous example, the representations were only constrained by the size of the hidden layer (32). In such a situation, what typically happens is that the hidden layer is learning an approximation of PCA (principal component analysis). But another way to constrain the representations to be compact is to add a sparsity contraint on the activity of the hidden representations, so fewer units would \"fire\" at a given time. In Keras, this can be done by adding an activity_regularizer to our Dense layer:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51CFrftbmXRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers\n",
        "\n",
        "encoding_dim = 32\n",
        "\n",
        "input_img = Input(shape=(784,))\n",
        "# add a Dense layer with a L1 activity regularizer\n",
        "encoded = Dense(encoding_dim, activation='relu',\n",
        "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3MMdOwz3Ygs",
        "colab_type": "text"
      },
      "source": [
        "# Deep autoencoder\n",
        "We do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, such as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmOWFmsI3Xyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(128, activation='relu')(input_img)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "encoded = Dense(32, activation='relu')(encoded)\n",
        "\n",
        "decoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(decoded)\n",
        "decoded = Dense(784, activation='sigmoid')(decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyi3OX0EmXR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b4b2589-0fce-46bb-f14a-211e69f65752"
      },
      "source": [
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.6932 - val_loss: 0.6932\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6932 - val_loss: 0.6931\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6931 - val_loss: 0.6931\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6930 - val_loss: 0.6930\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6929 - val_loss: 0.6929\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6929 - val_loss: 0.6928\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6928 - val_loss: 0.6927\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6927 - val_loss: 0.6927\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6926 - val_loss: 0.6926\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6926 - val_loss: 0.6925\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6925 - val_loss: 0.6924\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6924 - val_loss: 0.6924\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6923 - val_loss: 0.6923\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6923 - val_loss: 0.6922\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6922 - val_loss: 0.6921\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6921 - val_loss: 0.6921\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6920 - val_loss: 0.6920\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6920 - val_loss: 0.6919\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6919 - val_loss: 0.6918\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6918 - val_loss: 0.6918\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6917 - val_loss: 0.6917\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6917 - val_loss: 0.6916\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6916 - val_loss: 0.6915\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6915 - val_loss: 0.6915\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6914 - val_loss: 0.6914\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6914 - val_loss: 0.6913\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6913 - val_loss: 0.6912\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6912 - val_loss: 0.6912\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6911 - val_loss: 0.6911\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6911 - val_loss: 0.6910\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6910 - val_loss: 0.6909\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6909 - val_loss: 0.6909\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6908 - val_loss: 0.6908\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.6908 - val_loss: 0.6907\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6907 - val_loss: 0.6906\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6906 - val_loss: 0.6906\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6905 - val_loss: 0.6905\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6904 - val_loss: 0.6904\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6904 - val_loss: 0.6903\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6903 - val_loss: 0.6903\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6902 - val_loss: 0.6902\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6901 - val_loss: 0.6901\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6901 - val_loss: 0.6900\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6900 - val_loss: 0.6899\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6899 - val_loss: 0.6899\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 5s 91us/sample - loss: 0.6898 - val_loss: 0.6898\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6897 - val_loss: 0.6897\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6897 - val_loss: 0.6896\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6896 - val_loss: 0.6895\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6895 - val_loss: 0.6895\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6894 - val_loss: 0.6894\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6893 - val_loss: 0.6893\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6893 - val_loss: 0.6892\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6892 - val_loss: 0.6891\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6891 - val_loss: 0.6891\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6890 - val_loss: 0.6890\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 5s 91us/sample - loss: 0.6889 - val_loss: 0.6889\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 5s 90us/sample - loss: 0.6888 - val_loss: 0.6888\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6887 - val_loss: 0.6887\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6887 - val_loss: 0.6886\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6886 - val_loss: 0.6885\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6885 - val_loss: 0.6885\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6884 - val_loss: 0.6884\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6883 - val_loss: 0.6883\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6882 - val_loss: 0.6882\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6881 - val_loss: 0.6881\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6880 - val_loss: 0.6880\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6879 - val_loss: 0.6879\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.6879 - val_loss: 0.6878\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6878 - val_loss: 0.6877\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6877 - val_loss: 0.6876\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.6876 - val_loss: 0.6875\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6875 - val_loss: 0.6874\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6874 - val_loss: 0.6873\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6873 - val_loss: 0.6872\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6872 - val_loss: 0.6871\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6871 - val_loss: 0.6870\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6870 - val_loss: 0.6869\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6869 - val_loss: 0.6868\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.6868 - val_loss: 0.6867\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6867 - val_loss: 0.6866\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6866 - val_loss: 0.6865\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.6864 - val_loss: 0.6864\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6863 - val_loss: 0.6863\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6862 - val_loss: 0.6862\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6861 - val_loss: 0.6861\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.6860 - val_loss: 0.6859\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6859 - val_loss: 0.6858\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6857 - val_loss: 0.6857\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6856 - val_loss: 0.6856\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6855 - val_loss: 0.6854\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6854 - val_loss: 0.6853\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6852 - val_loss: 0.6852\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6851 - val_loss: 0.6850\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6850 - val_loss: 0.6849\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.6848 - val_loss: 0.6848\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.6847 - val_loss: 0.6846\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6846 - val_loss: 0.6845\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6844 - val_loss: 0.6843\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.6843 - val_loss: 0.6842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f92e2a43be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz-vpqZhmXR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder=Model(input_img,encoded)  # This model maps an input to its encoded  representation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghc7FdaamXSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoded_input=Input(shape=(encoding_dim,))\n",
        "# decoder_layer=autoencoder.layers[-1]\n",
        "# decoder=Model(encoded_input,decoder_layer(encoded_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w4yssgSmXSD",
        "colab_type": "code",
        "colab": {},
        "outputId": "3e336123-0153-4831-e963-16fcf578a6c0"
      },
      "source": [
        "encoded_imgs=encoder.predict(x_test)\n",
        "decoded_imgs=decoder.predict(encoded_imgs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i\n",
            "love\n",
            "you\n",
            "madan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkRoy6tzmXSI",
        "colab_type": "code",
        "colab": {},
        "outputId": "ad8da0a6-adff-4c48-9149-e9165051607c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "n=10\n",
        "plt.figure(figsize=(20,4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax=plt.subplot(2,n,i+1)\n",
        "  plt.imshow(x_test[i].reshape(28,28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax=plt.subplot(2,n,i+1+n)\n",
        "  plt.imshow(decoded_imgs[i].reshape(28,28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGAFxdEKmXSM",
        "colab_type": "code",
        "colab": {},
        "outputId": "4fb4200a-0664-41e5-9dbc-1bb0e3e6b475"
      },
      "source": [
        "number=input('enter any 10 number')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter any 10 number1,2,3,4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3JsuNf_mXSR",
        "colab_type": "code",
        "colab": {},
        "outputId": "11430d5e-94bb-4e76-94d2-02d1536a989b"
      },
      "source": [
        "ls=[]\n",
        "greater=0\n",
        "ls=number.split()\n",
        "for i in ls:\n",
        "     if int(i)>greater: \n",
        "            greater=i\n",
        "print(greater)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: '1,2,3,4'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-20-2df753759c10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m      \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mgreater\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mgreater\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreater\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '1,2,3,4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVkKI7wwmXSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}